@article{fuzzing_methods,
	author = {Mallissery, Sanoop and Wu, Yu-Sung},
	title = {Demystify the Fuzzing Methods: A Comprehensive Survey},
	year = {2023},
	issue_date = {March 2024},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {56},
	number = {3},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3623375},
	doi = {10.1145/3623375},
	abstract = {Massive software applications possess complex data structures or parse complex data structures; in such cases, vulnerabilities in the software become inevitable. The vulnerabilities are the source of cyber-security threats, and discovering this before the software deployment is challenging. Fuzzing is a vulnerability discovery solution that resonates with random-mutation, feedback-driven, coverage-guided, constraint-guided, seed-scheduling, and target-oriented strategies. Each technique is wrapped beneath the black-, white-, and grey-box fuzzers to uncover diverse vulnerabilities. It consists of methods such as identifying structural information about the test cases to detect security vulnerabilities, symbolic and concrete program states to explore the unexplored locations, and full semantics of code coverage to create new test cases. We methodically examine each kind of fuzzers and contemporary fuzzers with a profound observation that addresses various research questions and systematically reviews and analyze the gaps and their solutions. Our survey comprised the recent related works on fuzzing techniques to demystify the fuzzing methods concerning the application domains and the target that, in turn, achieves higher code coverage and sound vulnerability detection.},
	journal = {ACM Comput. Surv.},
	month = oct,
	articleno = {71},
	numpages = {38},
	keywords = {Automated testing, fuzzing, code inspection, vulnerability discovery}
}

@misc{schloegel_sok_2024,
	title = {{SoK}: {Prudent} {Evaluation} {Practices} for {Fuzzing}},
	shorttitle = {{SoK}},
	url = {http://arxiv.org/abs/2405.10220},
	doi = {10.1109/SP54263.2024.00137},
	abstract = {Fuzzing has proven to be a highly effective approach to uncover software bugs over the past decade. After AFL popularized the groundbreaking concept of lightweight coverage feedback, the field of fuzzing has seen a vast amount of scientific work proposing new techniques, improving methodological aspects of existing strategies, or porting existing methods to new domains. All such work must demonstrate its merit by showing its applicability to a problem, measuring its performance, and often showing its superiority over existing works in a thorough, empirical evaluation. Yet, fuzzing is highly sensitive to its target, environment, and circumstances, e.g., randomness in the testing process. After all, relying on randomness is one of the core principles of fuzzing, governing many aspects of a fuzzer's behavior. Combined with the often highly difficult to control environment, the reproducibility of experiments is a crucial concern and requires a prudent evaluation setup. To address these threats to validity, several works, most notably Evaluating Fuzz Testing by Klees et al., have outlined how a carefully designed evaluation setup should be implemented, but it remains unknown to what extent their recommendations have been adopted in practice. In this work, we systematically analyze the evaluation of 150 fuzzing papers published at the top venues between 2018 and 2023. We study how existing guidelines are implemented and observe potential shortcomings and pitfalls. We find a surprising disregard of the existing guidelines regarding statistical tests and systematic errors in fuzzing evaluations. For example, when investigating reported bugs, ...},
	urldate = {2024-05-21},
	author = {Schloegel, Moritz and Bars, Nils and Schiller, Nico and Bernhard, Lukas and Scharnowski, Tobias and Crump, Addison and Ebrahim, Arash Ale and Bissantz, Nicolai and Muench, Marius and Holz, Thorsten},
	month = may,
	year = {2024},
	note = {arXiv:2405.10220 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Software Engineering},
}

@inproceedings{gorz_systematic_2023,
	title = {Systematic {Assessment} of {Fuzzers} using {Mutation} {Analysis}},
	isbn = {978-1-939133-37-3},
	url = {https://www.usenix.org/conference/usenixsecurity23/presentation/gorz},
	language = {en},
	urldate = {2024-05-21},
	author = {Görz, Philipp and Mathis, Björn and Hassler, Keno and Güler, Emre and Holz, Thorsten and Zeller, Andreas and Gopinath, Rahul},
	year = {2023},
	pages = {4535--4552},
}

@misc{iotfuzzer,
	title = {{IoTFuzzer}: {Discovering} {Memory} {Corruptions} in {IoT} {Through} {App}-based {Fuzzing} {\textbar} {Welcome}},
	booktitle = {{NDSS}},
	author = {J. Chen and W. Diao and Q. Zhao and C. Zuo and Z. Lin and X. Wang and W. Lau and M. Sun and R. Yang and K. Zhang},
	year = {2018},
	url = {https://homes.luddy.indiana.edu/xw7/publication/iotfuzzer_ndss2018/},
	urldate = {2024-05-21},
}

@article{zhan_toward_2024,
	title = {Toward {Automated} {Field} {Semantics} {Inference} for {Binary} {Protocol} {Reverse} {Engineering}},
	volume = {19},
	issn = {1556-6021},
	url = {https://ieeexplore.ieee.org/document/10291000/keywords#keywords},
	doi = {10.1109/TIFS.2023.3326666},
	abstract = {Network protocol reverse engineering is the basis for many security applications. A common class of protocol reverse engineering methods is based on the analysis of network message traces. After performing message field identification by segmenting messages into multiple fields, a key task is to infer the semantics of the fields. One of the limitations of existing field semantics inference methods is that they usually infer semantics for only a few fields and often require a lot of manual effort. In this paper, we propose an automated field semantics inference method for binary protocol reverse engineering (FSIBP). FSIBP aims to automatically learn semantics inference knowledge from known protocols and use it to infer the semantics of any field of an unknown protocol. To achieve this goal, we design a feature extraction method that can extract features of the field itself and of the field context. We also propose a semantic category aggregation method that abstracts the fine-grained semantics of all fields of known protocols into aggregated semantic categories. Moreover, we make FSIBP infer semantics based on the similarity of fields to semantic categories. The above design enables FSIBP to utilize the semantic knowledge of all fields of known protocols and infer the semantics of any fields of unknown protocols. The whole process of FSIBP does not require any expert knowledge or manual parameter setting. We conduct extensive experiments to demonstrate the effectiveness of FSIBP. Moreover, we find a utility for FSIBP besides field semantics inference, its output can help to detect the mis-segmented fields generated during the message field identification.},
	urldate = {2024-05-21},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Zhan, Mengqi and Li, Yang and Li, Bo and Zhang, Jinchao and Li, Chuanrong and Wang, Weiping},
	year = {2024},
	note = {Conference Name: IEEE Transactions on Information Forensics and Security},
	keywords = {binary protocol reverse, Data mining, Feature extraction, field semantics inference, Manuals, Protocol reverse engineering, Protocols, Reverse engineering, Security, Semantics},
	pages = {764--776},
}

@article{tao_bit-oriented_2016,
	title = {Bit-oriented format extraction approach for automatic binary protocol reverse engineering},
	volume = {10},
	copyright = {© 2021 The Institution of Engineering and Technology},
	issn = {1751-8636},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-com.2015.0797},
	doi = {10.1049/iet-com.2015.0797},
	abstract = {Protocol message format extraction is a principal process of automatic network protocol reverse engineering when target protocol specifications are not available. However, binary protocol reverse engineering has been a new challenge in recent years for approaches that traditionally have dealt with text-based protocols rather than binary protocols. In this study, the authors propose a novel approach called PRE-Bin that automatically extracts binary-type fields of binary protocols based on fine-grained bits. First, a silhouette coefficient is introduced into the hierarchical clustering to confirm the optimal clustering number of binary frames. Second, a modified multiple sequence alignment algorithm, in which the matching process and back-tracing rules are redesigned, is also proposed to analyse binary field features. Finally, a Bayes decision model is invoked to describe field features and determine bit-oriented field boundaries. The maximum a posteriori criterion is leveraged to complete an optimal protocol format estimation of binary field boundaries. The authors implemented a prototype system of PRE-Bin to infer the specification of binary protocols from actual traffic traces. Experimental results indicate that PRE-Bin effectively extracts binary fields and outperforms the existing algorithms.},
	language = {en},
	number = {6},
	urldate = {2024-05-21},
	journal = {IET Communications},
	author = {Tao, Siyu and Yu, Hongyi and Li, Qing},
	year = {2016},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1049/iet-com.2015.0797},
	keywords = {automatic binary protocol reverse engineering, back-tracing rules, Bayes decision model, Bayes methods, binary field boundaries, binary field features, binary protocols, binary-type fields, bit-oriented field boundaries, bit-oriented format extraction approach, fine grained bits, optimal clustering number, optimal protocol format estimation, protocol message format extraction, reverse engineering, silhouette coefficient, target protocol specifications, text-based protocols},
	pages = {709--716},
}

@article{eceiza_fuzzing_2021,
	title = {Fuzzing the {Internet} of {Things}: {A} {Review} on the {Techniques} and {Challenges} for {Efficient} {Vulnerability} {Discovery} in {Embedded} {Systems}},
	volume = {8},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {2327-4662, 2372-2541},
	shorttitle = {Fuzzing the {Internet} of {Things}},
	url = {https://ieeexplore.ieee.org/document/9344712/},
	doi = {10.1109/JIOT.2021.3056179},
	abstract = {With a growing number of embedded devices that create, transform, and send data autonomously at its core, the Internet of Things (IoT) is a reality in different sectors, such as manufacturing, healthcare, or transportation. With this expansion, the IoT is becoming more present in critical environments, where security is paramount. Infamous attacks, such as Mirai, have shown the insecurity of the devices that power the IoT, as well as the potential of such large-scale attacks. Therefore, it is important to secure these embedded systems that form the backbone of the IoT. However, the particular nature of these devices and their resource constraints mean that the most cost-effective manner of securing these devices is to secure them before they are deployed, by minimizing the number of vulnerabilities they ship. To this end, fuzzing has proved itself as a valuable technique for automated vulnerability ﬁnding, where specially crafted inputs are fed to programs in order to trigger vulnerabilities and crash the system. In this survey, we link the world of embedded IoT devices and fuzzing. For this end, we list the particularities of the embedded world as far as security is concerned, we perform a literature review on fuzzing techniques and proposals, studying their applicability to embedded IoT devices and, ﬁnally, we present future research directions by pointing out the gaps identiﬁed in the review.},
	language = {en},
	number = {13},
	urldate = {2024-05-21},
	journal = {IEEE Internet of Things Journal},
	author = {Eceiza, Maialen and Flores, Jose Luis and Iturbe, Mikel},
	shortauthor = {Eceiza et al.},
	month = jul,
	year = {2021},
	pages = {10390--10411},
}

@misc{jiang_survey_2024,
	title = {A {Survey} of {Network} {Protocol} {Fuzzing}: {Model}, {Techniques} and {Directions}},
	shorttitle = {A {Survey} of {Network} {Protocol} {Fuzzing}},
	url = {http://arxiv.org/abs/2402.17394},
	abstract = {As one of the most successful and effective software testing techniques in recent years, fuzz testing has uncovered numerous bugs and vulnerabilities in modern software, including network protocol software. In contrast to other fuzzing targets, network protocol software exhibits its distinct characteristics and challenges, introducing a plethora of research questions that need to be addressed in the design and implementation of network protocol fuzzers. While some research work has evaluated and systematized the knowledge of general fuzzing techniques at a high level, there is a lack of similar analysis and summarization for fuzzing research specific to network protocols. This paper offers a comprehensive exposition of network protocol software’s fuzzing-related features and conducts a systematic review of some representative advancements in network protocol fuzzing since its inception. We summarize state-of-the-art strategies and solutions in various aspects, propose a unified protocol fuzzing process model, and introduce the techniques involved in each stage of the model. At the same time, this paper also summarizes the promising research directions in the landscape of protocol fuzzing to foster exploration within the community for more efficient and intelligent modern network protocol fuzzing techniques.},
	language = {en},
	urldate = {2024-05-27},
	publisher = {arXiv},
	author = {Jiang, Shihao and Zhang, Yu and Li, Junqiang and Yu, Hongfang and Luo, Long and Sun, Gang},
	month = feb,
	year = {2024},
	note = {arXiv:2402.17394 [cs]},
	keywords = {Computer Science - Networking and Internet Architecture},
}

@misc{noauthor_embedded_nodate,
	title = {Embedded fuzzing: a review of challenges, tools, and solutions {\textbar} {Cybersecurity} {\textbar} {Full} {Text}},
	url = {https://cybersecurity.springeropen.com/articles/10.1186/s42400-022-00123-y},
	urldate = {2024-05-27},
}

@article{eisele_embedded_2022,
	title = {Embedded fuzzing: a review of challenges, tools, and solutions},
	volume = {5},
	issn = {2523-3246},
	shorttitle = {Embedded fuzzing},
	url = {https://doi.org/10.1186/s42400-022-00123-y},
	doi = {10.1186/s42400-022-00123-y},
	abstract = {Fuzzing has become one of the best-established methods to uncover software bugs. Meanwhile, the market of embedded systems, which binds the software execution tightly to the very hardware architecture, has grown at a steady pace, and that pace is anticipated to become yet more sustained in the near future. Embedded systems also benefit from fuzzing, but the innumerable existing architectures and hardware peripherals complicate the development of general and usable approaches, hence a plethora of tools have recently appeared. Here comes a stringent need for a systematic review in the area of fuzzing approaches for embedded systems, which we term “embedded fuzzing” for brevity. The inclusion criteria chosen in this article are semi-objective in their coverage of the most relevant publication venues as well as of our personal judgement. The review rests on a formal definition we develop to represent the realm of embedded fuzzing. It continues by discussing the approaches that satisfy the inclusion criteria, then defines the relevant elements of comparison and groups the approaches according to how the execution environment is served to the system under test. The resulting review produces a table with 42 entries, which in turn supports discussion suggesting vast room for future research due to the limitations noted.},
	number = {1},
	urldate = {2024-05-27},
	journal = {Cybersecurity},
	author = {Eisele, Max and Maugeri, Marcello and Shriwas, Rachna and Huth, Christopher and Bella, Giampaolo},
	month = sep,
	year = {2022},
	keywords = {Dynamic analysis, Embedded security, Embedded systems, Software security, Vulnerability mining},
	pages = {18},
}

@book{herbert_linux_2004,
	address = {Hingham, Mass},
	edition = {1st ed},
	series = {Charles {River} {Media} programming series},
	title = {The {Linux} {TCP}/{IP} stack: networking for embedded systems},
	isbn = {978-1-58450-284-5},
	shorttitle = {The {Linux} {TCP}/{IP} stack},
	language = {en},
	publisher = {Charles River Media},
	author = {Herbert, Thomas F.},
	year = {2004},
	keywords = {Embedded Internet devices, Linux, Operating systems (Computers), TCP/IP (Computer network protocol)},
}

@incollection{thuraisingham_pulsar_2015,
	address = {Cham},
	title = {Pulsar: {Stateful} {Black}-{Box} {Fuzzing} of {Proprietary} {Network} {Protocols}},
	volume = {164},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-319-28864-2 978-3-319-28865-9},
	shorttitle = {Pulsar},
	url = {http://link.springer.com/10.1007/978-3-319-28865-9_18},
	abstract = {The security of network services and their protocols critically depends on minimizing their attack surface. A single ﬂaw in an implementation can sufﬁce to compromise a service and expose sensitive data to an attacker. The discovery of vulnerabilities in protocol implementations, however, is a challenging task: While for standard protocols this process can be conducted with regular techniques for auditing, the situation becomes difﬁcult for proprietary protocols if neither the program code nor the speciﬁcation of the protocol are easily accessible. As a result, vulnerabilities in closed-source implementations can often remain undiscovered for a longer period of time. In this paper, we present PULSAR, a method for stateful black-box fuzzing of proprietary network protocols. Our method combines concepts from fuzz testing with techniques for automatic protocol reverse engineering and simulation. It proceeds by observing the trafﬁc of a proprietary protocol and inferring a generative model for message formats and protocol states that can not only analyze but also simulate communication. During fuzzing this simulation can effectively explore the protocol state space and thereby enables uncovering vulnerabilities deep inside the protocol implementation. We demonstrate the efﬁcacy of PULSAR in two case studies, where it identiﬁes known as well as unknown vulnerabilities.},
	language = {en},
	urldate = {2024-06-26},
	booktitle = {Security and {Privacy} in {Communication} {Networks}},
	publisher = {Springer International Publishing},
	author = {Gascon, Hugo and Wressnegger, Christian and Yamaguchi, Fabian and Arp, Daniel and Rieck, Konrad},
	editor = {Thuraisingham, Bhavani and Wang, XiaoFeng and Yegneswaran, Vinod},
	year = {2015},
	doi = {10.1007/978-3-319-28865-9_18},
	note = {Series Title: Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering},
	pages = {330--347},
}

@inproceedings{pham_aflnet_2020,
	address = {Porto, Portugal},
	title = {{AFLNET}: {A} {Greybox} {Fuzzer} for {Network} {Protocols}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-72815-778-8},
	shorttitle = {{AFLNET}},
	url = {https://ieeexplore.ieee.org/document/9159093/},
	doi = {10.1109/ICST46399.2020.00062},
	abstract = {Server fuzzing is difﬁcult. Unlike simple commandline tools, servers feature a massive state space that can be traversed effectively only with well-deﬁned sequences of input messages. Valid sequences are speciﬁed in a protocol. In this paper, we present AFLNET, the ﬁrst greybox fuzzer for protocol implementations. Unlike existing protocol fuzzers, AFLNET takes a mutational approach and uses state-feedback to guide the fuzzing process. AFLNET is seeded with a corpus of recorded message exchanges between the server and an actual client. No protocol speciﬁcation or message grammars are required. AFLNET acts as a client and replays variations of the original sequence of messages sent to the server and retains those variations that were effective at increasing the coverage of the code or state space. To identify the server states that are exercised by a message sequence, AFLNET uses the server’s response codes. From this feedback, AFLNET identiﬁes progressive regions in the state space, and systematically steers towards such regions. The case studies with AFLNET on two popular protocol implementations demonstrate a substantial performance boost over the state-ofthe-art. AFLNET discovered two new CVEs which are classiﬁed as critical (CVSS score CRITICAL 9.8).},
	language = {en},
	urldate = {2024-08-23},
	booktitle = {2020 {IEEE} 13th {International} {Conference} on {Software} {Testing}, {Validation} and {Verification} ({ICST})},
	publisher = {IEEE},
	author = {Pham, Van-Thuan and Bohme, Marcel and Roychoudhury, Abhik},
	month = oct,
	year = {2020},
	pages = {460--465},
}

@misc{feng_snipuzz_2021,
	title = {Snipuzz: {Black}-box {Fuzzing} of {IoT} {Firmware} via {Message} {Snippet} {Inference}},
	shorttitle = {Snipuzz},
	url = {http://arxiv.org/abs/2105.05445},
	doi = {10.48550/arXiv.2105.05445},
	abstract = {The proliferation of Internet of Things (IoT) devices has made people's lives more convenient, but it has also raised many security concerns. Due to the difficulty of obtaining and emulating IoT firmware, the black-box fuzzing of IoT devices has become a viable option. However, existing black-box fuzzers cannot form effective mutation optimization mechanisms to guide their testing processes, mainly due to the lack of feedback. It is difficult or even impossible to apply existing grammar-based fuzzing strategies. Therefore, an efficient fuzzing approach with syntax inference is required in the IoT fuzzing domain. To address these critical problems, we propose a novel automatic black-box fuzzing for IoT firmware, termed Snipuzz. Snipuzz runs as a client communicating with the devices and infers message snippets for mutation based on the responses. Each snippet refers to a block of consecutive bytes that reflect the approximate code coverage in fuzzing. This mutation strategy based on message snippets considerably narrows down the search space to change the probing messages. We compared Snipuzz with four state-of-the-art IoT fuzzing approaches, i.e., IoTFuzzer, BooFuzz, Doona, and Nemesys. Snipuzz not only inherits the advantages of app-based fuzzing (e.g., IoTFuzzer, but also utilizes communication responses to perform efficient mutation. Furthermore, Snipuzz is lightweight as its execution does not rely on any prerequisite operations, such as reverse engineering of apps. We also evaluated Snipuzz on 20 popular real-world IoT devices. Our results show that Snipuzz could identify 5 zero-day vulnerabilities, and 3 of them could be exposed only by Snipuzz. All the newly discovered vulnerabilities have been confirmed by their vendors.},
	urldate = {2024-09-09},
	publisher = {arXiv},
	author = {Feng, Xiaotao and Sun, Ruoxi and Zhu, Xiaogang and Xue, Minhui and Wen, Sheng and Liu, Dongxi and Nepal, Surya and Xiang, Yang},
	month = may,
	year = {2021},
	note = {arXiv:2105.05445 [cs]},
	keywords = {Computer Science - Cryptography and Security, D.2.5, J.7},
}

@inproceedings{dolan-gavitt_lava_2016,
	title = {{LAVA}: {Large}-{Scale} {Automated} {Vulnerability} {Addition}},
	shorttitle = {{LAVA}},
	url = {https://ieeexplore.ieee.org/document/7546498},
	doi = {10.1109/SP.2016.15},
	abstract = {Work on automating vulnerability discovery has long been hampered by a shortage of ground-truth corpora with which to evaluate tools and techniques. This lack of ground truth prevents authors and users of tools alike from being able to measure such fundamental quantities as miss and false alarm rates. In this paper, we present LAVA, a novel dynamic taint analysis-based technique for producing ground-truth corpora by quickly and automatically injecting large numbers of realistic bugs into program source code. Every LAVA bug is accompanied by an input that triggers it whereas normal inputs are extremely unlikely to do so. These vulnerabilities are synthetic but, we argue, still realistic, in the sense that they are embedded deep within programs and are triggered by real inputs. Using LAVA, we have injected thousands of bugs into eight real-world programs, including bash, tshark, and the GNU coreutils. In a preliminary evaluation, we found that a prominent fuzzer and a symbolic execution-based bug finder were able to locate some but not all LAVA-injected bugs, and that interesting patterns and pathologies were already apparent in their performance. Our work forms the basis of an approach for generating large ground-truth vulnerability corpora on demand, enabling rigorous tool evaluation and providing a high-quality target for tool developers.},
	urldate = {2024-09-08},
	booktitle = {2016 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Dolan-Gavitt, Brendan and Hulin, Patrick and Kirda, Engin and Leek, Tim and Mambretti, Andrea and Robertson, Wil and Ulrich, Frederick and Whelan, Ryan},
	month = may,
	year = {2016},
	note = {ISSN: 2375-1207},
	keywords = {Security, Computer bugs, Geophysical measurement techniques, Ground penetrating radar, Length measurement, Pathology, Privacy},
	pages = {110--121},
}

@inproceedings{chen_towards_2022,
	title = {Towards {Effective} {Performance} {Fuzzing}},
	url = {https://ieeexplore.ieee.org/document/9985101},
	doi = {10.1109/ISSREW55968.2022.00055},
	abstract = {Fuzzing is an automated testing technique that utilizes injection of random inputs in a target program to help uncover vulnerabilities. Performance fuzzing extends the classic fuzzing approach and generates inputs that trigger poor performance. During our evaluation of performance fuzzing tools, we have identified certain conventionally used assumptions that do not always hold true. Our research (re)evaluates PERFFUZZ [1] in order to identify the limitations of current techniques, and guide the direction of future work for improvements to performance fuzzing. Our experimental results highlight two specific limitations. Firstly, we identify the assumption that the length of execution paths correlate to program performance is not always the case, and thus cannot reflect the quality of test cases generated by performance fuzzing. Secondly, the default testing parameters by the fuzzing process (timeouts and size limits) overly confine the input search space. Based on these observations, we suggest further investigation on performance fuzzing guidance, as well as controlled fuzzing and testing parameters.},
	urldate = {2024-09-08},
	booktitle = {2022 {IEEE} {International} {Symposium} on {Software} {Reliability} {Engineering} {Workshops} ({ISSREW})},
	author = {Chen, Yiqun and Bradbury, Matthew and Suri, Neeraj},
	month = oct,
	year = {2022},
	keywords = {Aerospace electronics, Conferences, Fuzzing, input selection, metrics, performance fuzzing, Software reliability, Testing},
	pages = {128--129},
}

@article{garshasbi_cnnpre_2023,
	title = {{CNNPRE}: {A} {CNN}-{Based} {Protocol} {Reverse} {Engineering} {Method}},
	volume = {11},
	issn = {2169-3536},
	shorttitle = {{CNNPRE}},
	url = {https://ieeexplore.ieee.org/document/10287339/?arnumber=10287339},
	doi = {10.1109/ACCESS.2023.3325391},
	abstract = {Given the growth in computer networks and Internet usage, the traditional network environment has evolved into a more intricate system. Many applications utilize unknown communication protocols, for which the specification documentation is not available. The use of undocumented network protocols raises various security and management concerns. Protocol reverse engineering based on network traffic aims to infer the behavior and format of unknown network protocols. Clustering same-type messages or packets is a crucial initial step in correctly performing reverse engineering of protocol syntax or behavior. Therefore, this paper proposes a new method called CNNPRE, utilizing deep learning techniques to identify and group traffic message types. Our method employs network traffic and traffic features as input. Specifically, we use convolutional neural networks and deep transfer learning for feature extraction and message type identification and to tackle the challenge of unlabeled training data in the real world scenarios of protocol reverse engineering. The experimental results demonstrate that our proposed method works well and outperforms other methods for different protocols and achieves an average Homogeneity score of more than 0.87 on all datasets. This means that the method can identify message types according to the changing characteristics of messages and traffic features without the need for human expert intervention.},
	urldate = {2024-09-08},
	journal = {IEEE Access},
	author = {Garshasbi, Javad and Teimouri, Mehdi},
	year = {2023},
	note = {Conference Name: IEEE Access},
	keywords = {Feature extraction, Protocols, Reverse engineering, Clustering algorithms, Convolutional neural networks, deep learning, deep transfer learning, message type identification, network protocols, protocol reverse engineering, Telecommunication traffic, Transfer learning},
	pages = {116255--116268},
}

@article{kim_evaluating_2024,
	title = {Evaluating {Directed} {Fuzzers}: {Are} {We} {Heading} in the {Right} {Direction}?},
	volume = {1},
	issn = {2994-970X},
	shorttitle = {Evaluating {Directed} {Fuzzers}},
	url = {https://dl.acm.org/doi/10.1145/3643741},
	doi = {10.1145/3643741},
	abstract = {TAE EUN KIM, KAIST, Korea JAESEUNG CHOI∗, Sogang University, Korea SEONGJAE IM, KAIST, Korea KIHONG HEO, KAIST, Korea SANG KIL CHA, KAIST, Korea Directed fuzzing recently has gained signiﬁcant attention due to its ability to reconstruct proof-of-concept (PoC) test cases for target code such as buggy lines or functions. Surprisingly, however, there has been no in-depth study on the way to properly evaluate directed fuzzers despite much progress in the ﬁeld. In this paper, we present the ﬁrst systematic study on the evaluation of directed fuzzers. In particular, we analyze common pitfalls in evaluating directed fuzzers with extensive experiments on ﬁve state-of-the-art tools, which amount to 30 CPU-years of computational eﬀort, in order to conﬁrm that diﬀerent choices made at each step of the evaluation process can signiﬁcantly impact the results. For example, we ﬁnd that a small change in the crash triage logic can substantially aﬀect the measured performance of a directed fuzzer, while the majority of the papers we studied do not fully disclose their crash triage scripts. We argue that disclosing the whole evaluation process is essential for reproducing research and facilitating future work in the ﬁeld of directed fuzzing. In addition, our study reveals that several common evaluation practices in the current directed fuzzing literature can mislead the overall assessments. Thus, we identify such mistakes in previous papers and propose guidelines for evaluating directed fuzzers. CCS Concepts: • Software and its engineering → Software testing and debugging; • Security and privacy → Software and application security.},
	language = {en},
	number = {FSE},
	urldate = {2024-09-07},
	journal = {Proceedings of the ACM on Software Engineering},
	author = {Kim, Tae Eun and Choi, Jaeseung and Im, Seongjae and Heo, Kihong and Cha, Sang Kil},
	month = jul,
	year = {2024},
	pages = {316--337},
}

@article{eceiza_improving_2023,
	title = {Improving fuzzing assessment methods through the analysis of metrics and experimental conditions},
	volume = {124},
	issn = {0167-4048},
	url = {https://www.sciencedirect.com/science/article/pii/S0167404822003388},
	doi = {10.1016/j.cose.2022.102946},
	abstract = {Fuzzing is nowadays one of the most widely used bug hunting techniques. By automatically generating malformed inputs, fuzzing aims to trigger unwanted behavior on its target. While fuzzing research has matured considerably in the last years, the evaluation and comparison of different fuzzing proposals remain challenging, as no standard set of metrics, data, or experimental conditions exist to allow such observation. This paper aims to fill that gap by proposing a standard set of features to allow such comparison. For that end, it first reviews the existing evaluation methods in the literature and discusses all existing metrics by evaluating seven fuzzers under identical experimental conditions. After examining the obtained results, it recommends a set of practices –particularly on the metrics to be used–, to allow proper comparison between different fuzzing proposals.},
	urldate = {2024-09-07},
	journal = {Computers \& Security},
	author = {Eceiza, Maialen and Flores, Jose Luis and Iturbe, Mikel},
	month = jan,
	year = {2023},
	keywords = {Security, Fuzzing, Evaluation methodology, Metrics, Software testing},
	pages = {102946},
}

@misc{noauthor_ai_nodate,
	title = {{AI} {Based} {Framework} for {Automatic} {Test} {Data} {Generation}},
}

@inproceedings{bohme_directed_2017,
	address = {Dallas Texas USA},
	title = {Directed {Greybox} {Fuzzing}},
	isbn = {978-1-4503-4946-8},
	url = {https://dl.acm.org/doi/10.1145/3133956.3134020},
	doi = {10.1145/3133956.3134020},
	abstract = {Existing Greybox Fuzzers (GF) cannot be effectively directed, for instance, towards problematic changes or patches, towards critical system calls or dangerous locations, or towards functions in the stacktrace of a reported vulnerability that we wish to reproduce.},
	language = {en},
	urldate = {2024-08-30},
	booktitle = {Proceedings of the 2017 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Böhme, Marcel and Pham, Van-Thuan and Nguyen, Manh-Dung and Roychoudhury, Abhik},
	month = oct,
	year = {2017},
	pages = {2329--2344},
}

@inproceedings{bekrar_finding_2011,
	title = {Finding {Software} {Vulnerabilities} by {Smart} {Fuzzing}},
	url = {https://ieeexplore.ieee.org/document/5770635},
	doi = {10.1109/ICST.2011.48},
	abstract = {Nowadays, one of the most effective ways to identify software vulnerabilities by testing is the use of fuzzing, whereby the robustness of software is tested against invalid inputs that play on implementation limits or data boundaries. A high number of random combinations of such inputs are sent to the system through its interfaces. Although fuzzing is a fast technique which detects real errors, its efficiency should be improved. Indeed, the main drawbacks of fuzz testing are its poor coverage which involves missing many errors, and the quality of tests. Enhancing fuzzing with advanced approaches such as: data tainting and coverage analysis would improve its efficiency and make it smarter. This paper will present an idea on how these techniques when combined give better error detection by iteratively guiding executions and generating the most pertinent test cases able to trigger potential vulnerabilities and maximize the coverage of testing.},
	urldate = {2024-08-30},
	booktitle = {Verification and {Validation} 2011 {Fourth} {IEEE} {International} {Conference} on {Software} {Testing}},
	author = {Bekrar, Sofia and Bekrar, Chaouki and Groz, Roland and Mounier, Laurent},
	month = mar,
	year = {2011},
	note = {ISSN: 2159-4848},
	keywords = {Security, Testing, Algorithm design and analysis, Assembly, fuzzing, Instruments, Monitoring, Software, software vulnerabilities, testing},
	pages = {427--430},
}

@inproceedings{chen_learning-guided_2019,
	title = {Learning-{Guided} {Network} {Fuzzing} for {Testing} {Cyber}-{Physical} {System} {Defences}},
	url = {http://arxiv.org/abs/1909.05410},
	doi = {10.1109/ASE.2019.00093},
	abstract = {The threat of attack faced by cyber-physical systems (CPSs), especially when they play a critical role in automating public infrastructure, has motivated research into a wide variety of attack defence mechanisms. Assessing their effectiveness is challenging, however, as realistic sets of attacks to test them against are not always available. In this paper, we propose smart fuzzing, an automated, machine learning guided technique for systematically ﬁnding ‘test suites’ of CPS network attacks, without requiring any knowledge of the system’s control programs or physical processes. Our approach uses predictive machine learning models and metaheuristic search algorithms to guide the fuzzing of actuators so as to drive the CPS into different unsafe physical states. We demonstrate the efﬁcacy of smart fuzzing by implementing it for two real-world CPS testbeds—a water puriﬁcation plant and a water distribution system—ﬁnding attacks that drive them into 27 different unsafe states involving water ﬂow, pressure, and tank levels, including six that were not covered by an established attack benchmark. Finally, we use our approach to test the effectiveness of an invariant-based defence system for the water treatment plant, ﬁnding two attacks that were not detected by its physical invariant checks, highlighting a potential weakness that could be exploited in certain conditions.},
	language = {en},
	urldate = {2024-08-30},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Chen, Yuqi and Poskitt, Christopher M. and Sun, Jun and Adepu, Sridhar and Zhang, Fan},
	month = nov,
	year = {2019},
	note = {arXiv:1909.05410 [cs]},
	keywords = {Computer Science - Cryptography and Security},
	pages = {962--973},
}

@article{li_fuzzing_2018,
	title = {Fuzzing: a survey},
	volume = {1},
	issn = {2523-3246},
	shorttitle = {Fuzzing},
	url = {https://doi.org/10.1186/s42400-018-0002-y},
	doi = {10.1186/s42400-018-0002-y},
	abstract = {Security vulnerability is one of the root causes of cyber-security threats. To discover vulnerabilities and fix them in advance, researchers have proposed several techniques, among which fuzzing is the most widely used one. In recent years, fuzzing solutions, like AFL, have made great improvements in vulnerability discovery. This paper presents a summary of the recent advances, analyzes how they improve the fuzzing process, and sheds light on future work in fuzzing. Firstly, we discuss the reason why fuzzing is popular, by comparing different commonly used vulnerability discovery techniques. Then we present an overview of fuzzing solutions, and discuss in detail one of the most popular type of fuzzing, i.e., coverage-based fuzzing. Then we present other techniques that could make fuzzing process smarter and more efficient. Finally, we show some applications of fuzzing, and discuss new trends of fuzzing and potential future directions.},
	number = {1},
	urldate = {2024-08-30},
	journal = {Cybersecurity},
	author = {Li, Jun and Zhao, Bodong and Zhang, Chao},
	month = jun,
	year = {2018},
	keywords = {Software security, Fuzzing, Coverage-based fuzzing, Vulnerability discovery},
	pages = {6},
}

@misc{noauthor_cysecbooksfuzzing_nodate,
	title = {{CySecBooks}/{Fuzzing} {Brute} {Force} {Vulnerability} {Discovery}.pdf at master · mangonugen/{CySecBooks}},
	url = {https://github.com/mangonugen/CySecBooks/blob/master/Fuzzing%20Brute%20Force%20Vulnerability%20Discovery.pdf},
	abstract = {Libros de Cyber Security. Contribute to mangonugen/CySecBooks development by creating an account on GitHub.},
	language = {en},
	urldate = {2024-08-30},
	journal = {GitHub},
}

@article{liang_fuzzing_2018,
	title = {Fuzzing: {State} of the {Art}},
	volume = {67},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0018-9529, 1558-1721},
	shorttitle = {Fuzzing},
	url = {https://ieeexplore.ieee.org/document/8371326/},
	doi = {10.1109/TR.2018.2834476},
	abstract = {As one of the most popular software testing techniques, fuzzing can ﬁnd a variety of weaknesses in a program, such as software bugs and vulnerabilities, by generating numerous test inputs. Due to its effectiveness, fuzzing is regarded as a valuable bug hunting method. In this paper, we present an overview of fuzzing that concentrates on its general process, as well as classiﬁcations, followed by detailed discussion of the key obstacles and some stateof-the-art technologies which aim to overcome or mitigate these obstacles. We further investigate and classify several widely used fuzzing tools. Our primary goal is to equip the stakeholder with a better understanding of fuzzing and the potential solutions for improving fuzzing methods in the spectrum of software testing and security. To inspire future research, we also predict some future directions with regard to fuzzing.},
	language = {en},
	number = {3},
	urldate = {2024-08-30},
	journal = {IEEE Transactions on Reliability},
	author = {Liang, Hongliang and Pei, Xiaoxiao and Jia, Xiaodong and Shen, Wuwei and Zhang, Jian},
	month = sep,
	year = {2018},
	pages = {1199--1218},
}

@misc{godefroid2017learnfuzzmachinelearninginput,
	title={Learn\&Fuzz: Machine Learning for Input Fuzzing},
	author={Patrice Godefroid and Hila Peleg and Rishabh Singh},
	year={2017},
	eprint={1701.07232},
	archivePrefix={arXiv},
	primaryClass={cs.AI},
	url={https://arxiv.org/abs/1701.07232},
}

@online{fuzzing-paper,
	author = {Cheng Wen },
	title = {Recent Papers Related To Fuzzing},
	url = {https://github.com/wcventure/FuzzingPaper},
}

@online{afl,
	author = {Michal Salewski},
	title = {AFL},
	year = {2013},
	url = {https://lcamtuf.coredump.cx/afl/},
	urldate = {2025-01-16},
}

@online{zzuf,
	author = {Sam Hocevar},
	title = {zzuf},
	year = {2006},
	url = {http://caca.zoy.org/wiki/zzuf},
	urldate = {2025-01-16},
}

@inproceedings{chen_angora_2018,
	address = {San Francisco, CA},
	title = {Angora: {Efficient} {Fuzzing} by {Principled} {Search}},
	isbn = {978-1-5386-4353-2},
	shorttitle = {Angora},
	url = {https://ieeexplore.ieee.org/document/8418633/},
	doi = {10.1109/SP.2018.00046},
	abstract = {Fuzzing is a popular technique for ﬁnding software bugs. However, the performance of the state-of-the-art fuzzers leaves a lot to be desired. Fuzzers based on symbolic execution produce quality inputs but run slow, while fuzzers based on random mutation run fast but have difﬁculty producing quality inputs. We propose Angora, a new mutation-based fuzzer that outperforms the state-of-the-art fuzzers by a wide margin. The main goal of Angora is to increase branch coverage by solving path constraints without symbolic execution. To solve path constraints efﬁciently, we introduce several key techniques: scalable byte-level taint tracking, context-sensitive branch count, search based on gradient descent, and input length exploration. On the LAVA-M data set, Angora found almost all the injected bugs, found more bugs than any other fuzzer that we compared with, and found eight times as many bugs as the second-best fuzzer in the program who. Angora also found 103 bugs that the LAVA authors injected but could not trigger. We also tested Angora on eight popular, mature open source programs. Angora found 6, 52, 29, 40 and 48 new bugs in ﬁle, jhead, nm, objdump and size, respectively. We measured the coverage of Angora and evaluated how its key techniques contribute to its impressive performance.},
	language = {en},
	urldate = {2025-01-16},
	booktitle = {2018 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	publisher = {IEEE},
	author = {Chen, Peng and Chen, Hao},
	month = may,
	year = {2018},
	pages = {711--725},
}

@article{godefroid_sage_2012,
	title = {{SAGE}: whitebox fuzzing for security testing},
	volume = {55},
	issn = {0001-0782, 1557-7317},
	shorttitle = {{SAGE}},
	url = {https://dl.acm.org/doi/10.1145/2093548.2093564},
	doi = {10.1145/2093548.2093564},
	abstract = {SAGE has had a remarkable impact at Microsoft.},
	language = {en},
	number = {3},
	urldate = {2025-01-16},
	journal = {Communications of the ACM},
	author = {Godefroid, Patrice and Levin, Michael Y. and Molnar, David},
	month = mar,
	year = {2012},
	pages = {40--44},
}

@inproceedings{liang_sequence_2019,
	address = {Montreal, QC, Canada},
	title = {Sequence {Coverage} {Directed} {Greybox} {Fuzzing}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-72811-519-1},
	url = {https://ieeexplore.ieee.org/document/8813278/},
	doi = {10.1109/ICPC.2019.00044},
	abstract = {Existing directed fuzzers are not efﬁcient enough. Directed symbolic-execution-based whitebox fuzzers, e.g. BugRedux, spend lots of time on heavyweight program analysis and constraints solving at runtime. Directed greybox fuzzers, such as AFLGo, perform well at runtime, but considerable calculation during instrumentation phase hinders the overall performance.},
	language = {en},
	urldate = {2025-01-16},
	booktitle = {2019 {IEEE}/{ACM} 27th {International} {Conference} on {Program} {Comprehension} ({ICPC})},
	publisher = {IEEE},
	author = {Liang, Hongliang and Zhang, Yini and Yu, Yue and Xie, Zhuosi and Jiang, Lin},
	month = may,
	year = {2019},
	pages = {249--259},
}

@misc{saavedra_review_2019,
	title = {A {Review} of {Machine} {Learning} {Applications} in {Fuzzing}},
	url = {http://arxiv.org/abs/1906.11133},
	doi = {10.48550/arXiv.1906.11133},
	abstract = {Fuzzing has played an important role in improving software development and testing over the course of several decades. Recent research in fuzzing has focused on applications of machine learning (ML), offering useful tools to overcome challenges in the fuzzing process. This review surveys the current research in applying ML to fuzzing. Specifically, this review discusses successful applications of ML to fuzzing, briefly explores challenges encountered, and motivates future research to address fuzzing bottlenecks.},
	language = {en},
	urldate = {2024-12-11},
	publisher = {arXiv},
	author = {Saavedra, Gary J. and Rodhouse, Kathryn N. and Dunlavy, Daniel M. and Kegelmeyer, Philip W.},
	month = oct,
	year = {2019},
	note = {arXiv:1906.11133 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{wang_systematic_2020,
	title = {A systematic review of fuzzing based on machine learning techniques},
	volume = {15},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0237749},
	doi = {10.1371/journal.pone.0237749},
	abstract = {Security vulnerabilities play a vital role in network security system. Fuzzing technology is widely used as a vulnerability discovery technology to reduce damage in advance. However, traditional fuzzing techniques have many challenges, such as how to mutate input seed files, how to increase code coverage, and how to effectively bypass verification. Machine learning technology has been introduced as a new method into fuzzing test to alleviate these challenges. This paper reviews the research progress of using machine learning technology for fuzzing test in recent years, analyzes how machine learning improve the fuzz process and results, and sheds light on future work in fuzzing. Firstly, this paper discusses the reasons why machine learning techniques can be used for fuzzing scenarios and identifies six different stages in which machine learning have been used. Then this paper systematically study the machine learning based fuzzing models from selection of machine learning algorithm, pre-processing methods, datasets, evaluation metrics, and hyperparameters setting. Next, this paper assesses the performance of the machine learning models based on the frequently used evaluation metrics. The results of the evaluation prove that machine learning technology has an acceptable capability of categorize predictive for fuzzing. Finally, the comparison on capability of discovering vulnerabilities between traditional fuzzing tools and machine learning based fuzzing tools is analyzed. The results depict that the introduction of machine learning technology can improve the performance of fuzzing. However, there are still some limitations, such as unbalanced training samples and difficult to extract the characteristics related to vulnerabilities.},
	language = {en},
	number = {8},
	urldate = {2024-12-11},
	journal = {PLOS ONE},
	author = {Wang, Yan and Jia, Peng and Liu, Luping and Huang, Cheng and Liu, Zhonglin},
	editor = {Song, Tao},
	month = aug,
	year = {2020},
	pages = {e0237749},
}

@misc{zhong_neural_2022,
	title = {Neural {Network} {Guided} {Evolutionary} {Fuzzing} for {Finding} {Traffic} {Violations} of {Autonomous} {Vehicles}},
	url = {http://arxiv.org/abs/2109.06126},
	doi = {10.48550/arXiv.2109.06126},
	abstract = {Self-driving cars and trucks, autonomous vehicles (AVs), should not be accepted by regulatory bodies and the public until they have much higher conﬁdence in their safety and reliability — which can most practically and convincingly be achieved by testing. But existing testing methods are inadequate for checking the end-to-end behaviors of AV controllers against complex, real-world corner cases involving interactions with multiple independent agents such as pedestrians and human-driven vehicles. While test-driving AVs on streets and highways fails to capture many rare events, existing simulation-based testing methods mainly focus on simple scenarios and do not scale well for complex driving situations that require sophisticated awareness of the surroundings. To address these limitations, we propose a new fuzz testing technique, called AutoFuzz, which can leverage widely-used AV simulators’ API grammars to generate semantically and temporally valid complex driving scenarios (sequences of scenes). To efﬁciently search for trafﬁc violations-inducing scenarios in a large search space, we propose a constrained neural network (NN) evolutionary search method to optimize AutoFuzz. Evaluation of our prototype on one state-of-the-art learning-based controller, two rule-based controllers, and one industrial-grade controller in ﬁve scenarios shows that AutoFuzz efﬁciently ﬁnds hundreds of trafﬁc violations in high-ﬁdelity simulation environments. For each scenario, AutoFuzz can ﬁnd on average 10-39\% more unique trafﬁc violations than the best-performing baseline method. Further, ﬁne-tuning the learning-based controller with the trafﬁc violations found by AutoFuzz successfully reduced the trafﬁc violations found in the new version of the AV controller software.},
	language = {en},
	urldate = {2024-12-11},
	publisher = {arXiv},
	author = {Zhong, Ziyuan and Kaiser, Gail and Ray, Baishakhi},
	month = jul,
	year = {2022},
	note = {arXiv:2109.06126 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics},
}

@inproceedings{she_mtfuzz_2020,
	title = {{MTFuzz}: {Fuzzing} with a {Multi}-{Task} {Neural} {Network}},
	shorttitle = {{MTFuzz}},
	url = {http://arxiv.org/abs/2005.12392},
	doi = {10.1145/3368089.3409723},
	abstract = {Fuzzing is a widely used technique for detecting software bugs and vulnerabilities. Most popular fuzzers generate new inputs using an evolutionary search to maximize code coverage. Essentially, these fuzzers start with a set of seed inputs, mutate them to generate new inputs, and identify the promising inputs using an evolutionary fitness function for further mutation.},
	language = {en},
	urldate = {2024-12-11},
	booktitle = {Proceedings of the 28th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	author = {She, Dongdong and Krishna, Rahul and Yan, Lu and Jana, Suman and Ray, Baishakhi},
	month = nov,
	year = {2020},
	note = {arXiv:2005.12392 [cs]},
	keywords = {Computer Science - Software Engineering},
	pages = {737--749},
	annote = {Comment: ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE) 2020},
}

@online{wachter_fuzzing,
	author = {Liam Wachter},
	title = {Fuzzing Schutzmaßnahmen},
	date = {2020-09-01},
	urldate = {2025-01-17},
	url = {https://wachter-space.de/2020/09/01/fuzzing_mitigations/},
}

@inproceedings{noller_badger_2018,
	title = {Badger: {Complexity} {Analysis} with {Fuzzing} and {Symbolic} {Execution}},
	shorttitle = {Badger},
	url = {http://arxiv.org/abs/1806.03283},
	doi = {10.1145/3213846.3213868},
	abstract = {Hybrid testing approaches that involve fuzz testing and symbolic execution have shown promising results in achieving high code coverage, uncovering subtle errors and vulnerabilities in a variety of software applications. In this paper we describe Badger - a new hybrid approach for complexity analysis, with the goal of discovering vulnerabilities which occur when the worst-case time or space complexity of an application is significantly higher than the average case. Badger uses fuzz testing to generate a diverse set of inputs that aim to increase not only coverage but also a resource-related cost associated with each path. Since fuzzing may fail to execute deep program paths due to its limited knowledge about the conditions that influence these paths, we complement the analysis with a symbolic execution, which is also customized to search for paths that increase the resource-related cost. Symbolic execution is particularly good at generating inputs that satisfy various program conditions but by itself suffers from path explosion. Therefore, Badger uses fuzzing and symbolic execution in tandem, to leverage their benefits and overcome their weaknesses. We implemented our approach for the analysis of Java programs, based on Kelinci and Symbolic PathFinder. We evaluated Badger on Java applications, showing that our approach is significantly faster in generating worst-case executions compared to fuzzing or symbolic execution on their own.},
	urldate = {2025-01-17},
	booktitle = {Proceedings of the 27th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	author = {Noller, Yannic and Kersten, Rody and Păsăreanu, Corina S.},
	month = jul,
	year = {2018},
	note = {arXiv:1806.03283 [cs]},
	keywords = {Computer Science - Software Engineering},
	pages = {322--332},
}

@misc{she_fox_2024,
	title = {{FOX}: {Coverage}-guided {Fuzzing} as {Online} {Stochastic} {Control}},
	shorttitle = {{FOX}},
	url = {http://arxiv.org/abs/2406.04517},
	doi = {10.48550/arXiv.2406.04517},
	abstract = {Fuzzing is an effective technique for discovering software vulnerabilities by generating random test inputs and executing them against the target program. However, fuzzing large and complex programs remains challenging due to difficulties in uncovering deeply hidden vulnerabilities. This paper addresses the limitations of existing coverage-guided fuzzers, focusing on the scheduler and mutator components. Existing schedulers suffer from information sparsity and the inability to handle fine-grained feedback metrics. The mutators are agnostic of target program branches, leading to wasted computation and slower coverage exploration. To overcome these issues, we propose an end-to-end online stochastic control formulation for coverage-guided fuzzing. Our approach incorporates a novel scheduler and custom mutator that can adapt to branch logic, maximizing aggregate edge coverage achieved over multiple stages. The scheduler utilizes fine-grained branch distance measures to identify frontier branches, where new coverage is likely to be achieved. The mutator leverages branch distance information to perform efficient and targeted seed mutations, leading to robust progress with minimal overhead. We present FOX, a proof-of-concept implementation of our control-theoretic approach, and compare it to industry-standard coverage-guided fuzzers. 6 CPU-years of extensive evaluations on the FuzzBench dataset and complex real-world programs (a total of 38 test programs) demonstrate that FOX outperforms existing state-of-the-art fuzzers, achieving average coverage improvements up to 26.45\% in real-world standalone programs and 6.59\% in FuzzBench programs over the state-of-the-art AFL++. In addition, it uncovers 20 unique bugs in popular real-world applications including eight that are previously unknown, showcasing real-world security impact.},
	urldate = {2025-01-17},
	publisher = {arXiv},
	author = {She, Dongdong and Storek, Adam and Xie, Yuchong and Kweon, Seoyoung and Srivastava, Prashast and Jana, Suman},
	month = jun,
	year = {2024},
	note = {arXiv:2406.04517 [cs]},
	keywords = {Computer Science - Cryptography and Security},
	annote = {Comment: To Appear in Proceedings of the 2024 ACM SIGSAC Conference on Computer and Communications Security (CCS '24)},
}

@online{afl_whitepaper,
	author = {Michal Zalewski},
	title = {Technical "whitepaper" for afl-fuzz},
	year = {2013},
	urldate = {2025-01-17},
	url = {https://lcamtuf.coredump.cx/afl/technical_details.txt},
}

@misc{deng_large_2023,
	title = {Large {Language} {Models} are {Zero}-{Shot} {Fuzzers}: {Fuzzing} {Deep}-{Learning} {Libraries} via {Large} {Language} {Models}},
	shorttitle = {Large {Language} {Models} are {Zero}-{Shot} {Fuzzers}},
	url = {http://arxiv.org/abs/2212.14834},
	doi = {10.48550/arXiv.2212.14834},
	abstract = {Detecting bugs in Deep Learning (DL) libraries (e.g., TensorFlow/PyTorch) is critical for almost all downstream DL systems in ensuring effectiveness/safety for end users. Meanwhile, traditional fuzzing techniques can be hardly effective for such a challenging domain since the input DL programs need to satisfy both the input language (e.g., Python) syntax/semantics and the DL API input/shape constraints for tensor computations. To address these limitations, we propose TitanFuzz - the first approach to directly leveraging Large Language Models (LLMs) to generate input programs for fuzzing DL libraries. LLMs are titanic models trained on billions of code snippets and can auto-regressively generate human-like code snippets. Our key insight is that modern LLMs can also include numerous code snippets invoking DL library APIs in their training corpora, and thus can implicitly learn both language syntax/semantics and intricate DL API constraints for valid DL program generation. More specifically, we use both generative and infilling LLMs (e.g., Codex/InCoder) to generate and mutate valid/diverse input DL programs for fuzzing. Our experimental results demonstrate that TitanFuzz can achieve 30.38\%/50.84\% higher code coverage than state-of-the-art fuzzers on TensorFlow/PyTorch. Furthermore, TitanFuzz is able to detect 65 bugs, with 41 already confirmed as previously unknown bugs. This paper demonstrates that modern titanic LLMs can be leveraged to directly perform both generation-based and mutation-based fuzzing studied for decades, while being fully automated, generalizable, and applicable to domains challenging for traditional approaches (such as DL systems). We hope TitanFuzz can stimulate more work in this promising direction of LLMs for fuzzing.},
	urldate = {2025-01-18},
	publisher = {arXiv},
	author = {Deng, Yinlin and Xia, Chunqiu Steven and Peng, Haoran and Yang, Chenyuan and Zhang, Lingming},
	month = mar,
	year = {2023},
	note = {arXiv:2212.14834 [cs]},
	keywords = {Computer Science - Software Engineering},
	annote = {Comment: Accepted at ISSTA 2023},
}

@inproceedings{devlin_bert_2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://aclanthology.org/N19-1423/},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2025-01-18},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	month = jun,
	year = {2019},
	pages = {4171--4186},
}

@inproceedings{paduraru_riverfuzzrl_2021,
	title = {{RiverFuzzRL} - an open-source tool to experiment with reinforcement learning for fuzzing},
	url = {https://ieeexplore.ieee.org/abstract/document/9438567},
	doi = {10.1109/ICST49551.2021.00055},
	abstract = {Combining fuzzing techniques and reinforcement learning could be an important direction in software testing. However, there is a gap in support for experimentation in this field, as there are no open-source tools to let academia and industry to perform experiments easily. The purpose of this paper is to fill this gap by introducing a new framework, named RiverFuzzRL, on top of our already mature frame-work for AI-guided fuzzing, River. We provide out-of-the-box implementations for users to choose from or customize for their test target. The work presented here is performed on testing binaries and does not require access to the source code, but it can be easily adapted to other types of software testing as well. We also discuss the challenges faced, opportunities, and factors that are important for performance, as seen in the evaluation.},
	urldate = {2025-01-18},
	booktitle = {2021 14th {IEEE} {Conference} on {Software} {Testing}, {Verification} and {Validation} ({ICST})},
	author = {Paduraru, Ciprian and Paduraru, Miruna and Stefanescu, Alin},
	month = apr,
	year = {2021},
	note = {ISSN: 2159-4848},
	keywords = {binary analysis, Conferences, fuzzing, Fuzzing, Industries, open-source tool, Protocols, reinforcement learning, Reinforcement learning, Search problems, symbolic execution, Tools},
	pages = {430--435},
}
@misc{ramadan_role_2024,
	title = {The {Role} of {Artificial} {Intelligence} and {Machine} {Learning} in {Software} {Testing}},
	url = {http://arxiv.org/abs/2409.02693},
	doi = {10.48550/arXiv.2409.02693},
	abstract = {Artificial Intelligence (AI) and Machine Learning (ML) have significantly impacted various industries, including software development. Software testing, a crucial part of the software development lifecycle (SDLC), ensures the quality and reliability of software products. Traditionally, software testing has been a labor-intensive process requiring significant manual effort. However, the advent of AI and ML has transformed this landscape by introducing automation and intelligent decision-making capabilities. AI and ML technologies enhance the efficiency and effectiveness of software testing by automating complex tasks such as test case generation, test execution, and result analysis. These technologies reduce the time required for testing and improve the accuracy of defect detection, ultimately leading to higher quality software. AI can predict potential areas of failure by analyzing historical data and identifying patterns, which allows for more targeted and efficient testing. This paper explores the role of AI and ML in software testing by reviewing existing literature, analyzing current tools and techniques, and presenting case studies that demonstrate the practical benefits of these technologies. The literature review provides a comprehensive overview of the advancements in AI and ML applications in software testing, highlighting key methodologies and findings from various studies. The analysis of current tools showcases the capabilities of popular AI-driven testing tools such as Eggplant AI, Test.ai, Selenium, Appvance, Applitools Eyes, Katalon Studio, and Tricentis Tosca, each offering unique features and advantages. Case studies included in this paper illustrate real-world applications of AI and ML in software testing, showing significant improvements in testing efficiency, accuracy, and overall software quality.},
	urldate = {2025-01-18},
	publisher = {arXiv},
	author = {Ramadan, Ahmed and Yasin, Husam and Pektas, Burhan},
	month = sep,
	year = {2024},
	note = {arXiv:2409.02693 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@inproceedings {AFLplusplus-Woot20,
	author = {Andrea Fioraldi and Dominik Maier and Heiko Ei{\ss}feldt and Marc Heuse},
	title = {{AFL++}: Combining Incremental Steps of Fuzzing Research},
	booktitle = {14th {USENIX} Workshop on Offensive Technologies ({WOOT} 20)},
	year = {2020},
	publisher = {{USENIX} Association},
	month = aug,
}

@inproceedings{fioraldi_libafl_2022,
	address = {New York, NY, USA},
	series = {{CCS} '22},
	title = {{LibAFL}: {A} {Framework} to {Build} {Modular} and {Reusable} {Fuzzers}},
	isbn = {978-1-4503-9450-5},
	shorttitle = {{LibAFL}},
	url = {https://dl.acm.org/doi/10.1145/3548606.3560602},
	doi = {10.1145/3548606.3560602},
	abstract = {The release of AFL marked an important milestone in the area of software security testing, revitalizing fuzzing as a major research topic and spurring a large number of research studies that attempted to improve and evaluate the different aspects of the fuzzing pipeline.Many of these studies implemented their techniques by forking the AFL codebase. While this choice might seem appropriate at first, combining multiple forks into a single fuzzer requires a high engineering overhead, which hinders progress in the area and prevents fair and objective evaluations of different techniques. The highly fragmented landscape of the fuzzing ecosystem also prevents researchers from combining orthogonal techniques and makes it difficult for end users to adopt new prototype solutions.To tackle this problem, in this paper we propose LibAFL, a framework to build modular and reusable fuzzers. We discuss the different components generally used in fuzzing and map them to an extensible framework. LibAFL allows researchers and engineers to extend the core fuzzer pipeline and share their new components for further evaluations. As part of LibAFL, we integrated techniques from more than 20 previous works and conduct extensive experiments to show the benefit of our framework to combine and evaluate different approaches. We hope this can help to shed light on current advancements in fuzzing and provide a solid base for comparative and extensible research in the future.},
	urldate = {2025-01-18},
	booktitle = {Proceedings of the 2022 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Fioraldi, Andrea and Maier, Dominik Christian and Zhang, Dongjia and Balzarotti, Davide},
	month = nov,
	year = {2022},
	pages = {1051--1065},
}

@inproceedings{schwartz_all_2010,
	title = {All {You} {Ever} {Wanted} to {Know} about {Dynamic} {Taint} {Analysis} and {Forward} {Symbolic} {Execution} (but {Might} {Have} {Been} {Afraid} to {Ask})},
	url = {https://ieeexplore.ieee.org/document/5504796},
	doi = {10.1109/SP.2010.26},
	abstract = {Dynamic taint analysis and forward symbolic execution are quickly becoming staple techniques in security analyses. Example applications of dynamic taint analysis and forward symbolic execution include malware analysis, input filter generation, test case generation, and vulnerability discovery. Despite the widespread usage of these two techniques, there has been little effort to formally define the algorithms and summarize the critical issues that arise when these techniques are used in typical security contexts. The contributions of this paper are two-fold. First, we precisely describe the algorithms for dynamic taint analysis and forward symbolic execution as extensions to the run-time semantics of a general language. Second, we highlight important implementation choices, common pitfalls, and considerations when using these techniques in a security context.},
	urldate = {2025-01-18},
	booktitle = {2010 {IEEE} {Symposium} on {Security} and {Privacy}},
	author = {Schwartz, Edward J. and Avgerinos, Thanassis and Brumley, David},
	month = may,
	year = {2010},
	note = {ISSN: 2375-1207},
	keywords = {Computerized monitoring, dynamic analysis, Filters, Heuristic algorithms, Information analysis, Information security, Performance analysis, Privacy, Reactive power, Runtime, symbolic execution, taint analysis, Testing},
	pages = {317--331},
}
